import logging
import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG
OWNER = "a.ryabichev"
DAG_ID = "raw_from_api_to_s3"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² DAG
LAYER = "raw"  # Ğ¡Ğ»Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: raw - ÑÑ‹Ñ€Ñ‹Ğµ, silver - Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ, gold - Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
SOURCE = "earthquake"  # Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ÑĞµÑ€Ğ²Ğ¸Ñ Ğ·ĞµĞ¼Ğ»ĞµÑ‚Ñ€ÑÑĞµĞ½Ğ¸Ğ¹ USGS

# S3 (MinIO) - Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ĞºĞ»ÑÑ‡Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğ¸Ğ· Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Airflow
ACCESS_KEY = Variable.get("access_key")  # Ğ›Ğ¾Ğ³Ğ¸Ğ½ Ğ´Ğ»Ñ MinIO (Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑÑ Ğ² ÑĞµĞºÑ€ĞµÑ‚Ğ°Ñ… Airflow)
SECRET_KEY = Variable.get("secret_key")  # ĞŸĞ°Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ»Ñ MinIO (Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑÑ Ğ² ÑĞµĞºÑ€ĞµÑ‚Ğ°Ñ… Airflow)

LONG_DESCRIPTION = """
# LONG DESCRIPTION
"""

SHORT_DESCRIPTION = "SHORT DESCRIPTION"

# ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ DAG Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 10, 20, tz="Europe/Moscow"),  
    "catchup": True,  # Airflow Ğ´Ğ¾Ğ³Ğ¾Ğ½Ğ¸Ñ‚ Ğ²ÑĞµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¸ Ñ start_date
    "retries": 3,  # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ
    "retry_delay": pendulum.duration(hours=1),  # Ğ–Ğ´Ğ°Ñ‚ÑŒ 1 Ñ‡Ğ°Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°Ğ¼Ğ¸
}


def get_dates(**context) -> tuple[str, str]:
    #ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ°Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Airflow
    start_date = context["data_interval_start"].format("YYYY-MM-DD")  
    end_date = context["data_interval_end"].format("YYYY-MM-DD")      

    return start_date, end_date


def get_and_transfer_api_data_to_s3(**context):
    """
    ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ: Ğ·Ğ°Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· API Ğ·ĞµĞ¼Ğ»ĞµÑ‚Ñ€ÑÑĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² S3
    ĞŸÑ€Ğ¾Ñ†ĞµÑÑ:
    1. ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´Ğ°Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ»Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
    2. ĞĞ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ DuckDB Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ S3
    3. Ğ§Ğ¸Ñ‚Ğ°ĞµÑ‚ CSV Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· USGS API
    4. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² Parquet Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ² MinIO
    """
    
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ´Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° DAG
    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for dates: {start_date}/{end_date}")
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ DuckDB (in-memory Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)
    con = duckdb.connect()

    # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ SQL Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ² DuckDB
    con.sql(
        f"""
        -- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ñ‡Ğ°ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑĞ°
        SET TIMEZONE='UTC';
        
        -- Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ HTTP Ğ¸ S3
        INSTALL httpfs;
        LOAD httpfs;
        
        -- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğº MinIO (S3-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ)
        SET s3_url_style = 'path';           
        SET s3_endpoint = 'minio:9000';      
        SET s3_access_key_id = '{ACCESS_KEY}';     
        SET s3_secret_access_key = '{SECRET_KEY}'; 
        SET s3_use_ssl = FALSE;             

        -- ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· API Ğ² S3
        COPY
        (
            SELECT
                *
            FROM
                -- Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ CSV Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· USGS API Ğ·Ğ° ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´
                read_csv_auto('https://earthquake.usgs.gov/fdsnws/event/1/query?format=csv&starttime={start_date}&endtime={end_date}') AS res
        ) 
        -- Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² MinIO Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Parquet
        TO 's3://prod/{LAYER}/{SOURCE}/{start_date}/{start_date}_00-00-00.gz.parquet';
        """,
    )

    con.close()  # Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ DuckDB
    logging.info(f"âœ… Download for date success: {start_date}")


# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ DAG Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ğ¼Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ
with DAG(
    dag_id=DAG_ID,
    schedule_interval="0 5 * * *",  # CRON Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ: ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ² 05:00 UTC
    default_args=args,
    tags=["s3", "raw"],  # Ğ¢ĞµĞ³Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Airflow UI
    description=SHORT_DESCRIPTION,
    concurrency=1,        # ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 1 ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ DAG
    max_active_tasks=1,   # ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 1 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°
    max_active_runs=1,    # ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 1 Ğ·Ğ°Ğ¿ÑƒÑĞº DAG
) as dag:
    dag.doc_md = LONG_DESCRIPTION  # Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Airflow UI

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ DAG
    start = EmptyOperator(task_id="start")  # ĞŸÑƒÑÑ‚Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°-Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ° Ğ´Ğ»Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ°

    # python_callable - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ-ÑÑÑ‹Ğ»ĞºĞ° Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
    # airflow ÑĞ°Ğ¼ Ğ²Ñ‹Ğ·Ğ¾Ğ²ĞµÑ‚ ÑÑ‚Ñƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ñ€Ğ¸Ğ´Ñ‘Ñ‚ Ğ²Ñ€ĞµĞ¼Ñ
    # Ğ¼Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ğ¼ Airflow ĞºĞ°ĞºÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ±ĞµĞ· () Ğ² ĞºĞ¾Ğ½Ñ†Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
    get_and_transfer_api_data_to_s3 = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,  # Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ±ÑƒĞ´ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒÑÑ
    )

    end = EmptyOperator(task_id="end")  # ĞŸÑƒÑÑ‚Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°-Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ° Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ†Ğ°

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡: start â†’ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â†’ end
    start >> get_and_transfer_api_data_to_s3 >> end




